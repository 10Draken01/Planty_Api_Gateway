/**
 * Servicio de Chat usando Ollama
 */

import { IChatService } from '@application/use-cases/SendMessageUseCase';
import { Ollama } from 'ollama';

export class OllamaChatService implements IChatService {
  private ollama: Ollama;
  private model: string;

  constructor(baseUrl?: string, model?: string) {
    this.ollama = new Ollama({
      host: baseUrl || process.env.OLLAMA_BASE_URL || 'http://localhost:11434'
    });
    this.model = model || process.env.OLLAMA_CHAT_MODEL || 'llama3.2';
  }

  async generateResponse(
    query: string,
    context: string
  ): Promise<string> {
    try {
      // Construir el prompt con contexto RAG
      const systemPrompt = this.buildSystemPrompt();
      const userPrompt = this.buildUserPrompt(query, context);

      // Preparar mensajes para Ollama
      const messages: Array<{ role: string; content: string }> = [
        {
          role: 'system',
          content: systemPrompt
        },
        {
          role: 'user',
          content: userPrompt
        }
      ];

      // Generar respuesta
      const response = await this.ollama.chat({
        model: this.model,
        messages: messages,
        stream: false,
        options: {
          temperature: 0.8,
          top_p: 0.9,
          top_k: 40
        }
      });

      if (!response.message || !response.message.content) {
        throw new Error('No se pudo generar una respuesta');
      }

      return response.message.content;
    } catch (error) {
      console.error('Error generando respuesta:', error);
      throw new Error(
        `Error al generar respuesta con Ollama: ${error instanceof Error ? error.message : 'Error desconocido'}`
      );
    }
  }

  private buildSystemPrompt(): string {
    return `Eres Planty üåø, un asistente virtual especializado en plantas de Suchiapa, Chiapas, M√©xico.

TU PERSONALIDAD:
- Eres alegre, entusiasta, en√©rgico y te ENCANTAN las plantas üå±
- Tienes un tono conversacional, cercano, divertido y amigable
- Usas emojis relevantes en tus respuestas para hacerlas m√°s amenas üòäüå∫üå∏üçÉ
- Te emociona compartir conocimientos sobre plantas con mucha energ√≠a
- Eres como ese amigo experto que siempre tiene datos curiosos sobre la naturaleza

INSTRUCCIONES IMPORTANTES:
- Responde SIEMPRE en espa√±ol de manera clara, coherente, amigable y con entusiasmo
- Estructura tus respuestas de forma organizada y f√°cil de leer
- Usa emojis de plantas, naturaleza y emociones para hacer tus respuestas m√°s expresivas
- Basa TODAS tus respuestas en el contexto proporcionado de la base de datos
- Si el contexto no tiene suficiente informaci√≥n, dilo de forma amigable y ofrece ayuda general
- S√© espec√≠fico con los nombres de las plantas, explicando de forma divertida y clara
- Comparte datos curiosos cuando sea relevante
- Si no est√°s seguro de algo, adm√≠telo con humor en lugar de inventar informaci√≥n
- Mant√©n las respuestas concisas pero informativas (2-5 p√°rrafos m√°ximo)
- A√±ade personalidad: usa expresiones como "¬°Qu√© emoci√≥n!", "¬°Me encanta esa planta!", "¬°Genial pregunta!", etc.
- S√© coherente y relevante con la pregunta del usuario
- No te desv√≠es del tema de plantas y jardiner√≠a`;
  }

  private buildUserPrompt(query: string, context: string): string {
    if (context && context.trim().length > 0) {
      return `De lo siguiente responde coherentemente, con energ√≠a, de manera clara, divertida y amigable.

üìö INFORMACI√ìN DISPONIBLE EN LA BASE DE DATOS:
${context}

üí¨ PREGUNTA DEL USUARIO:
"${query}"

INSTRUCCIONES PARA TU RESPUESTA:
- Lee cuidadosamente la pregunta del usuario y el contexto proporcionado
- Genera una respuesta coherente, directa y relevante a la pregunta espec√≠fica
- Usa √öNICAMENTE la informaci√≥n de la base de datos proporcionada arriba
- Estructura tu respuesta de forma clara y organizada
- S√© espec√≠fico y menciona detalles relevantes del contexto
- Mant√©n un tono entusiasta, amigable y divertido
- Usa emojis relevantes para hacer la respuesta m√°s amena
- Si la informaci√≥n en el contexto es limitada para responder completamente, menci√≥nalo de forma simp√°tica y ofrece lo que s√≠ sabes
- No inventes informaci√≥n que no est√© en el contexto proporcionado`;
    } else {
      return `De lo siguiente responde coherentemente, con energ√≠a, de manera clara, divertida y amigable.

üí¨ PREGUNTA DEL USUARIO:
"${query}"

‚ö†Ô∏è SITUACI√ìN: No encontr√© informaci√≥n espec√≠fica en mi base de datos sobre esta pregunta.

INSTRUCCIONES PARA TU RESPUESTA:
- Genera una respuesta amigable y honesta indicando que NO tienes informaci√≥n espec√≠fica sobre esa consulta en tu base de datos de plantas de Suchiapa
- Mant√©n un tono positivo, alegre y entusiasta
- Ofrece ayuda de forma general si es apropiado
- Sugiere al usuario que reformule su pregunta o pregunte sobre otras plantas
- Usa emojis para mantener la conversaci√≥n amena
- NO inventes informaci√≥n sobre plantas que no conoces`;
    }
  }

  /**
   * Genera una respuesta en streaming (para futuras implementaciones)
   */
  async generateResponseStream(
    query: string,
    context: string,
    onChunk: (chunk: string) => void
  ): Promise<void> {
    const systemPrompt = this.buildSystemPrompt();
    const userPrompt = this.buildUserPrompt(query, context);

    const stream = await this.ollama.chat({
      model: this.model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      stream: true
    });

    for await (const chunk of stream) {
      if (chunk.message?.content) {
        onChunk(chunk.message.content);
      }
    }
  }

  /**
   * Verifica si el modelo est√° disponible
   */
  async checkModelAvailability(): Promise<boolean> {
    try {
      const models = await this.ollama.list();
      return models.models.some((m) => m.name.includes(this.model));
    } catch (error) {
      console.error('Error verificando disponibilidad del modelo:', error);
      return false;
    }
  }

  /**
   * Obtiene informaci√≥n del modelo
   */
  getModelInfo(): { baseUrl: string; model: string } {
    return {
      baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
      model: this.model
    };
  }
}
